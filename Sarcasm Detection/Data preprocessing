import nltk
nltk.download('punkt')

def processText(text):
    text = text.lower()
    
    text = re.sub('((www.[^s]+)|(https?://[^s]+))','',text)
    text = re.sub('@[^s]+','',text)
    text = re.sub('[s]+', ' ', text)
    text = re.sub(r'#([^s]+)', '', text)
    
    tokenized_text = word_tokenize(text)
    print(tokenized_text)
    
    output_string = ''
    
    for each in tokenized_text:
        if ( each not in string.punctuation and each not in stopwords and len(each) > 1 ):
            output_string += each + ' '


    return output_string

df['text'][0]
processText(df['text'][0])

tqdm.pandas()
df['text'] = df['text'].progress_apply(processText)
count_length()

def tokenize_text(text):
    token_list = []
    for sent in nltk.sent_tokenize(text):
        for word in nltk.word_tokenize(sent):
            if ( len (word ) < 2 ):
                continue
            token_list.append(word)
    return token_list

train_tagged = df.progress_apply(lambda r:gensim.models.doc2vec.TaggedDocument(tokenize_text(r['text']), [r['label']]), axis=1)

def doc_2_vec_model(train_tagged_):
    train_set = []
    for tag in tqdm(train_tagged):
        train_set.append(tag)

    dbow_model = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=2, epochs=30)
    dbow_model.build_vocab([x for x in tqdm(train_set)])
    dbow_model.train(train_set, total_examples=len(train_set), epochs=50)
    return train_set, dbow_model

def vector_for_learning(model, input_docs):
    sents = input_docs
    feature_vectors, target = zip(*[(model.infer_vector(doc.words), doc.tags[0]) for doc in tqdm(sents) ])
    return feature_vectors, target

train_ds, dbow_model = doc_2_vec_model(train_tagged)
x, y = vector_for_learning(dbow_model, train_ds)
x = np.array(x)
y = np.array(y)

y = [1 if label == 'sarcastic' else 0 for label in y]
print(y)
